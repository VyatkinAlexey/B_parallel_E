{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, sorted_tokens, unknown_token_id=0):\n",
    "    words = text.strip().split()\n",
    "    words = [word + end_token for word in words]\n",
    "    words_string = ''.join(words)\n",
    "    \n",
    "    def _encode(string, token2id):\n",
    "        token2id = token2id.copy()\n",
    "        \n",
    "        if string == '':\n",
    "            return []\n",
    "        if len(token2id) == 0:\n",
    "            return [unknown_token_id]\n",
    "        \n",
    "        token, id = next(iter(token2id.items()))\n",
    "        token2id.popitem(last=False)\n",
    "        token_reg = re.escape(token)\n",
    "\n",
    "        string_tokens = []\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        \n",
    "        #print(matched_positions)\n",
    "        if len(matched_positions) == 0:\n",
    "            return _encode(string, token2id)\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            ts = _encode(substring, token2id)\n",
    "            string_tokens += ts\n",
    "            string_tokens += [id]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += _encode(remaining_substring, token2id)\n",
    "        \n",
    "        return string_tokens\n",
    "    \n",
    "    return _encode(words_string, sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(token_ids, id2token, unknown_token_id=0):\n",
    "    string = ''\n",
    "    for id in token_ids:\n",
    "        if id == unknown_token_id:\n",
    "            string += '<UNK>'\n",
    "        else:\n",
    "            token = id2token[id]\n",
    "            if token.endswith(end_token):\n",
    "                string += token[:-1] + ' '\n",
    "            else:\n",
    "                string += token\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('s@', 3), ('a@', 4), ('a', 1), ('s', 2), ('@', 5)])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id = OrderedDict({'a': 1, 's': 2, 's@': 3, 'a@': 4, '@': 5})\n",
    "token2id = OrderedDict(sorted(token2id.items(), reverse=True, key=lambda x: len(x[0])))\n",
    "id2token = {v: k for (k, v) in token2id.items()}\n",
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'd a asdsa d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 4, 1, 2, 0, 2, 4, 0, 5]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(s, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK> a as<UNK>sa <UNK>'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(s, token2id), id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tokenize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tokenize.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import copy\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-file', type=str)\n",
    "    parser.add_argument('-dict-file', type=str)\n",
    "    parser.add_argument('-unk-id', type=int, default=0)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.dict_file, 'rb') as f:\n",
    "        token2id = pickle.load(f)\n",
    "    with open(args.file, 'r') as f:\n",
    "        words = '\\n'.join(f.readlines()).strip().split()\n",
    "\n",
    "    words_string = ''.join(words)\n",
    "    unknown_token_id = args.unk_id \n",
    "\n",
    "    def encode(string, token2id):\n",
    "        token2id = token2id.copy()\n",
    "\n",
    "        if string == '':\n",
    "            return []\n",
    "        if len(token2id) == 0:\n",
    "            return [unknown_token_id]\n",
    "\n",
    "        token, id = next(iter(token2id.items()))\n",
    "        token2id.popitem(last=False)\n",
    "        token_reg = re.escape(token)\n",
    "\n",
    "        string_tokens = []\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        if len(matched_positions) == 0:\n",
    "            return encode(string, token2id)\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += encode(substring, token2id)\n",
    "            string_tokens += [id]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += encode(remaining_substring, token2id)\n",
    "\n",
    "        return string_tokens\n",
    "\n",
    "    ids = encode(words_string, token2id.copy())\n",
    "    print(ids)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tokenize_parallel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tokenize_parallel.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import copy\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    size = comm.Get_size()\n",
    "    rank = comm.Get_rank()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-file', type=str)\n",
    "    parser.add_argument('-dict-file', type=str)\n",
    "    parser.add_argument('-unk-id', type=int, default=0)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.dict_file, 'rb') as f:\n",
    "        token2id = pickle.load(f)\n",
    "    with open(args.file, 'r') as f:\n",
    "        words = '\\n'.join(f.readlines()).strip().split()\n",
    "\n",
    "    words_per_process = len(words) // size\n",
    "\n",
    "    if rank == size - 1:\n",
    "        words = words[rank * words_per_process:]\n",
    "    else:\n",
    "        words = words[rank * words_per_process:(rank + 1) * words_per_process]\n",
    "\n",
    "    words_string = ''.join(words)\n",
    "    unknown_token_id = args.unk_id \n",
    "\n",
    "    def encode(string, token2id):\n",
    "        token2id = token2id.copy()\n",
    "\n",
    "        if string == '':\n",
    "            return []\n",
    "        if len(token2id) == 0:\n",
    "            return [unknown_token_id]\n",
    "\n",
    "        token, id = next(iter(token2id.items()))\n",
    "        token2id.popitem(last=False)\n",
    "        token_reg = re.escape(token)\n",
    "\n",
    "        string_tokens = []\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        if len(matched_positions) == 0:\n",
    "            return encode(string, token2id)\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += encode(substring, token2id)\n",
    "            string_tokens += [id]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += encode(remaining_substring, token2id)\n",
    "\n",
    "        return string_tokens\n",
    "\n",
    "    ids = encode(words_string, token2id.copy())\n",
    "    ids = comm.gather(ids, root=0)\n",
    "    if rank == 0:\n",
    "        print(ids)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
