{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "log_reg_toxic_comment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SsmaEjculq_Q",
        "PQmOkjdJmS9l",
        "aYtzoX1tFLh6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjfqVZvMmfPr"
      },
      "source": [
        "## Toxic Comment Classification Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOaSzPSG2Q-n"
      },
      "source": [
        "Based on:\n",
        "- https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams\n",
        "- https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZxLPyoqIp56"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "import scipy\n",
        "from scipy import sparse"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q80xyKFS0JFY"
      },
      "source": [
        "# data from https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1aBA7lUyAB4YsfTAFHJDaFSOMZ3dYlmt4' -O test.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_X4qVCka54NIoTNGfgtA8lClnLaMEEo-' -O train.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHvtrvaLnFyY",
        "outputId": "75bef8f9-3bf9-4edb-f5ee-fd97edf21702"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " sample_data   test.csv   train2.csv  'view?usp=sharing'\n",
            " test2.csv     tets.csv   train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdH-uBX0nDT5"
      },
      "source": [
        "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "train = pd.read_csv('train.csv').fillna(' ')\n",
        "test = pd.read_csv('test.csv').fillna(' ')\n",
        "\n",
        "train_text = train['comment_text']\n",
        "test_text = test['comment_text']\n",
        "all_text = pd.concat([train_text, test_text])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN-JF05ouEFS"
      },
      "source": [
        "word_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1),\n",
        "    max_features=1000, dtype=np.float32) # 1000 and float32 for smaller size of the data\n",
        "\n",
        "word_vectorizer.fit(all_text)\n",
        "\n",
        "train_word_features = word_vectorizer.transform(train_text)\n",
        "test_word_features = word_vectorizer.transform(test_text)\n",
        "\n",
        "# char_vectorizer = TfidfVectorizer(\n",
        "#     sublinear_tf=True,\n",
        "#     strip_accents='unicode',\n",
        "#     analyzer='char',\n",
        "#     stop_words='english',\n",
        "#     ngram_range=(2, 6),\n",
        "#     max_features=5) #50000\n",
        "# char_vectorizer.fit(all_text)\n",
        "# train_char_features = char_vectorizer.transform(train_text)\n",
        "# test_char_features = char_vectorizer.transform(test_text)\n",
        "\n",
        "# train_features = hstack([train_char_features, train_word_features])\n",
        "# test_features = hstack([test_char_features, test_word_features])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHuX_DZSuQRs"
      },
      "source": [
        "train_features = train_word_features[:50000, :]\n",
        "test_features = test_word_features "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lItHziLQolrS",
        "outputId": "d3ba3023-d7c1-4bea-97d5-1c1ea75bd089"
      },
      "source": [
        "scores = []\n",
        "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name][:50000]\n",
        "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
        "    scores.append(cv_score)\n",
        "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
        "\n",
        "    classifier.fit(train_features, train_target)\n",
        "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
        "\n",
        "print('Total CV score is {}'.format(np.mean(scores)))\n",
        "\n",
        "# submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV score for class toxic is 0.9293452157933823\n",
            "CV score for class severe_toxic is 0.9703661637757519\n",
            "CV score for class obscene is 0.9672287752755669\n",
            "CV score for class threat is 0.9586053365497146\n",
            "CV score for class insult is 0.9481660654309895\n",
            "CV score for class identity_hate is 0.9526833173986237\n",
            "Total CV score is 0.9543991457040047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bupbuFS3x9nP"
      },
      "source": [
        "Hand-made log-reg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hIasdmlx8e7"
      },
      "source": [
        "class LogisticRegression2:\n",
        "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def __add_intercept(self, X):\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        return np.concatenate((intercept, X), axis=1)\n",
        "        # intercept = sparse.csr_matrix(np.ones((X.shape[0], 1)))\n",
        "        # return scipy.sparse.hstack([intercept, train_features])\n",
        "    \n",
        "    def __sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def __loss(self, h, y):\n",
        "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "        \n",
        "        # weights initialization\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        \n",
        "        for i in range(self.num_iter):\n",
        "            z = np.dot(X, self.theta)\n",
        "            h = self.__sigmoid(z)\n",
        "            gradient = np.dot(X.T, (h - y)) / y.size\n",
        "            self.theta -= self.lr * gradient\n",
        "            \n",
        "            z = np.dot(X, self.theta)\n",
        "            h = self.__sigmoid(z)\n",
        "            loss = self.__loss(h, y)\n",
        "                \n",
        "            if(self.verbose == True and i % 5000 == 0):\n",
        "                print(f'loss: {loss} \\t')\n",
        "    \n",
        "    def predict_prob(self, X):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "    \n",
        "        return self.__sigmoid(np.dot(X, self.theta))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.predict_prob(X).round()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsmaEjculq_Q"
      },
      "source": [
        "### Toy example - iris"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmzo1pLKzhba"
      },
      "source": [
        "from sklearn import datasets"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMqROTElzfpy"
      },
      "source": [
        "iris = sklearn.datasets.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = (iris.target != 0) * 1"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hemdQnRqz4WK",
        "outputId": "0adebc11-2409-48d3-e700-8173020d48b5"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150, 2), (150,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExmzbF8Rz8At",
        "outputId": "d5f0fa5e-83d5-490a-8564-d9e2328204f3"
      },
      "source": [
        "model = LogisticRegression2(lr=0.6, num_iter=5000)\n",
        "model.fit(X, y)\n",
        "preds = model.predict(X)\n",
        "(preds == y).mean()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX9x-qjfyEZA",
        "outputId": "6499a227-624d-4146-e88e-6e37b2ec7044"
      },
      "source": [
        "for class_name in class_names:\n",
        "    print(class_name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toxic\n",
            "severe_toxic\n",
            "obscene\n",
            "threat\n",
            "insult\n",
            "identity_hate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoZNUIbPoRv7"
      },
      "source": [
        "### Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pr0fFKNCknG"
      },
      "source": [
        "class_name = 'toxic'\n",
        "train_data = train_features.toarray().astype(np.float16)\n",
        "train_target = train[class_name][:50000]\n",
        "\n",
        "model = LogisticRegression2(lr=0.6, num_iter=5000)\n",
        "model.fit(train_data, train_target)\n",
        "\n",
        "preds = model.predict(train_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0oAa1RtHhOq",
        "outputId": "f0887a3c-62aa-433b-c0db-31019ebe1676"
      },
      "source": [
        "roc_auc_score(train_target, preds)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6621700909875433"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOO94Jerlgxq"
      },
      "source": [
        "### CuPy based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh-pmh-0_lTo"
      },
      "source": [
        "import cupy as cp \n",
        "\n",
        "class LogisticRegression2_cp:\n",
        "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def __add_intercept(self, X):\n",
        "        intercept = cp.ones((X.shape[0], 1))\n",
        "        return cp.concatenate((intercept, X), axis=1)\n",
        "    \n",
        "    def __sigmoid(self, z):\n",
        "        return 1 / (1 + cp.exp(-z))\n",
        "\n",
        "    def __loss(self, h, y):\n",
        "        return (-y * cp.log(h) - (1 - y) * cp.log(1 - h)).mean()\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "        \n",
        "        # weights initialization\n",
        "        self.theta = cp.zeros(X.shape[1])\n",
        "        \n",
        "        for i in range(self.num_iter):\n",
        "            z = cp.dot(X, self.theta)\n",
        "            h = self.__sigmoid(z)\n",
        "            gradient = cp.dot(X.T, (h - y)) / y.size\n",
        "            self.theta -= self.lr * gradient\n",
        "            \n",
        "            z = cp.dot(X, self.theta)\n",
        "            h = self.__sigmoid(z)\n",
        "            loss = self.__loss(h, y)\n",
        "                \n",
        "            if(self.verbose ==True and i % 5000 == 0):\n",
        "                print(f'loss: {loss} \\t')\n",
        "    \n",
        "    def predict_prob(self, X):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "    \n",
        "        return self.__sigmoid(cp.dot(X, self.theta))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.predict_prob(X).round()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QyhrQhuL3Et",
        "outputId": "abaed9d8-0d4a-47e0-dca3-09ab4278348a"
      },
      "source": [
        "class_name = 'toxic'\n",
        "\n",
        "train_data = cp.array(train_features.toarray().astype(np.float16))\n",
        "train_target = cp.array(train[class_name][:50000])\n",
        "\n",
        "model = LogisticRegression2_cp(lr=0.7, num_iter=50000, verbose=True)\n",
        "model.fit(train_data, train_target)\n",
        "preds = model.predict(train_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.5872211299226802 \t\n",
            "loss: 0.18030092109521675 \t\n",
            "loss: 0.16020582964766272 \t\n",
            "loss: 0.1514020123394156 \t\n",
            "loss: 0.14637800241508434 \t\n",
            "loss: 0.14312807241846429 \t\n",
            "loss: 0.14086036314354874 \t\n",
            "loss: 0.13919394249997266 \t\n",
            "loss: 0.13792176769776543 \t\n",
            "loss: 0.13692149817221402 \t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NBNfxLrr3mc",
        "outputId": "d2551e84-a3b1-44ad-a3f4-0b35b2e36425"
      },
      "source": [
        "roc_auc_score(cp.asnumpy(train_target), cp.asnumpy(preds))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7905050414102754"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOUd_2M41Lx5",
        "outputId": "79755db6-63d4-49f1-f231-7afe732cca54"
      },
      "source": [
        "class_name = 'toxic'\n",
        "\n",
        "train_data = cp.array(train_features.toarray().astype(np.float16))\n",
        "train_target = cp.array(train[class_name][:50000])\n",
        "\n",
        "model = LogisticRegression2_cp(lr=0.2, num_iter=300000, verbose=True)\n",
        "model.fit(train_data, train_target)\n",
        "preds = model.predict(train_data)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.6607707205136976 \t\n",
            "loss: 0.23159990911221284 \t\n",
            "loss: 0.20128802724835448 \t\n",
            "loss: 0.1856774080671144 \t\n",
            "loss: 0.175922517035376 \t\n",
            "loss: 0.16914512731159656 \t\n",
            "loss: 0.16411420768034918 \t\n",
            "loss: 0.16020870623413344 \t\n",
            "loss: 0.1570777766599001 \t\n",
            "loss: 0.15450626935612238 \t\n",
            "loss: 0.15235386045031074 \t\n",
            "loss: 0.150524566075961 \t\n",
            "loss: 0.1489502088996186 \t\n",
            "loss: 0.14758086030215523 \t\n",
            "loss: 0.1463790205413086 \t\n",
            "loss: 0.14531591999324736 \t\n",
            "loss: 0.14436908284789787 \t\n",
            "loss: 0.1435206730250885 \t\n",
            "loss: 0.14275634160199174 \t\n",
            "loss: 0.14206440535666093 \t\n",
            "loss: 0.14143524957345738 \t\n",
            "loss: 0.14086088617962228 \t\n",
            "loss: 0.14033462162884958 \t\n",
            "loss: 0.13985080371691885 \t\n",
            "loss: 0.1394046260851262 \t\n",
            "loss: 0.13899197550369977 \t\n",
            "loss: 0.1386093113042481 \t\n",
            "loss: 0.13825356926789498 \t\n",
            "loss: 0.1379220843259253 \t\n",
            "loss: 0.13761252788170877 \t\n",
            "loss: 0.13732285660495333 \t\n",
            "loss: 0.13705127030700348 \t\n",
            "loss: 0.1367961770631323 \t\n",
            "loss: 0.13655616416209254 \t\n",
            "loss: 0.1363299737744023 \t\n",
            "loss: 0.13611648246684074 \t\n",
            "loss: 0.13591468387120623 \t\n",
            "loss: 0.13572367395471832 \t\n",
            "loss: 0.13554263844780232 \t\n",
            "loss: 0.1353708420698882 \t\n",
            "loss: 0.13520761926083505 \t\n",
            "loss: 0.1350523661787804 \t\n",
            "loss: 0.13490453376771697 \t\n",
            "loss: 0.13476362173226256 \t\n",
            "loss: 0.1346291732846958 \t\n",
            "loss: 0.13450077055176404 \t\n",
            "loss: 0.13437803054708086 \t\n",
            "loss: 0.1342606016299563 \t\n",
            "loss: 0.13414816038387736 \t\n",
            "loss: 0.13404040885810165 \t\n",
            "loss: 0.13393707212433326 \t\n",
            "loss: 0.13383789610754804 \t\n",
            "loss: 0.13374264565597377 \t\n",
            "loss: 0.13365110282022072 \t\n",
            "loss: 0.13356306531576273 \t\n",
            "loss: 0.13347834514652313 \t\n",
            "loss: 0.13339676737033626 \t\n",
            "loss: 0.13331816898961818 \t\n",
            "loss: 0.13324239795276877 \t\n",
            "loss: 0.13316931225369794 \t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zqxM1KZ5COP",
        "outputId": "d95680a6-b592-4979-d877-087c2d908077"
      },
      "source": [
        "roc_auc_score(cp.asnumpy(train_target), cp.asnumpy(preds))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7994795323861603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTGa1ylFo6Xs"
      },
      "source": [
        "### Timing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgPsoRDsS5do",
        "outputId": "3136a204-5103-4af0-8c4d-68033ab2f99e"
      },
      "source": [
        "#gpu\n",
        "model = LogisticRegression2_cp(lr=0.6, num_iter=5000)\n",
        "train_data = cp.array(train_features.toarray().astype(np.float16))\n",
        "train_target = cp.array(train[class_name][:50000])\n",
        "%timeit -r 1 model.fit(train_data, train_target)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 1: 28.2 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VOKS8XnUJg0",
        "outputId": "4ba2a2fb-2035-455d-e5db-6dfc62f800e3"
      },
      "source": [
        "#cpu\n",
        "model = LogisticRegression2(lr=0.6, num_iter=5000)\n",
        "train_data = train_features.toarray().astype(np.float16)\n",
        "train_target = train[class_name][:50000]\n",
        "%timeit -r 1 model.fit(train_data, train_target)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 1: 8min 29s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf2WIupTUtqA"
      },
      "source": [
        "# CuPy version 18 times faster - збс"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQmOkjdJmS9l"
      },
      "source": [
        "## BPE from article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfynf4F82IGf"
      },
      "source": [
        "# https://leimao.github.io/blog/Byte-Pair-Encoding/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM6UFaz03LoG"
      },
      "source": [
        "import re, collections # https://docs.python.org/3/library/re.html\n",
        "\n",
        "def get_vocab(filename):\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
        "        for line in fhand:\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1): # for all pairs of two\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair)) # какая-то история с добавлением \\ - пробел что ли\n",
        "    #print(bigram)\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') # какая-то история с добавлением \\ - пробел что ли, \n",
        "    # https://docs.python.org/3/library/re.html#re.compile\n",
        "    #print(p)\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word) # https://docs.python.org/3/library/re.html#re.sub\n",
        "        #print(w_out)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def get_tokens(vocab):\n",
        "    tokens = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens[token] += freq\n",
        "    return tokens\n",
        "\n",
        "def get_tokens_from_vocab(vocab):\n",
        "    tokens_frequencies = collections.defaultdict(int)\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYtzoX1tFLh6"
      },
      "source": [
        "### Experients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBiyCY6x7TgG",
        "outputId": "f1fa4e6a-90a5-4fcb-fdf8-822dcc419e88"
      },
      "source": [
        "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
        "print('==========')\n",
        "print('Tokens Before BPE')\n",
        "tokens = get_tokens(vocab)\n",
        "print('Tokens: {}'.format(tokens))\n",
        "print('Number of tokens: {}'.format(len(tokens)))\n",
        "print('==========')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========\n",
            "Tokens Before BPE\n",
            "Tokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 17, 'r': 2, 'n': 6, 's': 9, 't': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 11\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNeKL_7T8hH8",
        "outputId": "7e142498-fd69-400d-9d38-71d9a82299f1"
      },
      "source": [
        "vocab"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'l o w </w>': 5,\n",
              " 'l o w e r </w>': 2,\n",
              " 'n e w e s t </w>': 6,\n",
              " 'w i d e s t </w>': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCyR-3y-8D_d",
        "outputId": "74033fb6-c738-4e67-9a91-16ad8d3badd3"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'</w>': 16,\n",
              "             'd': 3,\n",
              "             'e': 17,\n",
              "             'i': 3,\n",
              "             'l': 7,\n",
              "             'n': 6,\n",
              "             'o': 7,\n",
              "             'r': 2,\n",
              "             's': 9,\n",
              "             't': 9,\n",
              "             'w': 16})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeWTPKH28PkE",
        "outputId": "7e8620cb-b92b-4c1f-f870-981ab02ff26a"
      },
      "source": [
        "pairs = get_stats(vocab)\n",
        "pairs"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('d', 'e'): 3,\n",
              "             ('e', 'r'): 2,\n",
              "             ('e', 's'): 9,\n",
              "             ('e', 'w'): 6,\n",
              "             ('i', 'd'): 3,\n",
              "             ('l', 'o'): 7,\n",
              "             ('n', 'e'): 6,\n",
              "             ('o', 'w'): 7,\n",
              "             ('r', '</w>'): 2,\n",
              "             ('s', 't'): 9,\n",
              "             ('t', '</w>'): 9,\n",
              "             ('w', '</w>'): 5,\n",
              "             ('w', 'e'): 8,\n",
              "             ('w', 'i'): 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7duiLAR8ZeG",
        "outputId": "210a4908-e6ce-465e-cd2b-7c2ef7ee1cae"
      },
      "source": [
        "best = max(pairs, key=pairs.get)\n",
        "best"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e', 's')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kff5FpTAsBx",
        "outputId": "95047fed-124a-4bf4-eebf-fcdb04995fbf"
      },
      "source": [
        "num_merges = 10\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print('Iter: {}'.format(i))\n",
        "    print('Best pair: {}'.format(best))\n",
        "    tokens = get_tokens(vocab)\n",
        "    print('Tokens: {}'.format(tokens))\n",
        "    print('Number of tokens: {}'.format(len(tokens)))\n",
        "    print('==========')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0\n",
            "Best pair: ('l', 'o')\n",
            "Tokens: defaultdict(<class 'int'>, {'lo': 7, 'w': 16, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'est</w>': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 1\n",
            "Best pair: ('lo', 'w')\n",
            "Tokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'w': 9, 'est</w>': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 2\n",
            "Best pair: ('n', 'e')\n",
            "Tokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 2, 'r': 2, 'ne': 6, 'w': 9, 'est</w>': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 3\n",
            "Best pair: ('ne', 'w')\n",
            "Tokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 2, 'r': 2, 'new': 6, 'est</w>': 9, 'w': 3, 'i': 3, 'd': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 4\n",
            "Best pair: ('new', 'est</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 2, 'r': 2, 'newest</w>': 6, 'w': 3, 'i': 3, 'd': 3, 'est</w>': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 5\n",
            "Best pair: ('low', '</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'w': 3, 'i': 3, 'd': 3, 'est</w>': 3})\n",
            "Number of tokens: 10\n",
            "==========\n",
            "Iter: 6\n",
            "Best pair: ('w', 'i')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'wi': 3, 'd': 3, 'est</w>': 3})\n",
            "Number of tokens: 9\n",
            "==========\n",
            "Iter: 7\n",
            "Best pair: ('wi', 'd')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'wid': 3, 'est</w>': 3})\n",
            "Number of tokens: 8\n",
            "==========\n",
            "Iter: 8\n",
            "Best pair: ('wid', 'est</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'low': 2, 'e': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'widest</w>': 3})\n",
            "Number of tokens: 7\n",
            "==========\n",
            "Iter: 9\n",
            "Best pair: ('low', 'e')\n",
            "Tokens: defaultdict(<class 'int'>, {'low</w>': 5, 'lowe': 2, 'r': 2, '</w>': 2, 'newest</w>': 6, 'widest</w>': 3})\n",
            "Number of tokens: 6\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aItOxGMXAyXM",
        "outputId": "9fa5cbd8-dac4-43c8-c7e4-efcaa4553f40"
      },
      "source": [
        "vocab"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'low</w>': 5, 'lowe r </w>': 2, 'newest</w>': 6, 'widest</w>': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwLBfVEv7bji",
        "outputId": "a40eca53-b43c-41b0-8ce7-4d3801a3c8d9"
      },
      "source": [
        "num_merges = 3\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print('Iter: {}'.format(i))\n",
        "    print('Best pair: {}'.format(best))\n",
        "    tokens = get_tokens(vocab)\n",
        "    print('Tokens: {}'.format(tokens))\n",
        "    print('Number of tokens: {}'.format(len(tokens)))\n",
        "    print('==========')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e\\ s\n",
            "re.compile('(?<!\\\\S)e\\\\ s(?!\\\\S)')\n",
            "l o w </w>\n",
            "l o w e r </w>\n",
            "n e w es t </w>\n",
            "w i d es t </w>\n",
            "Iter: 0\n",
            "Best pair: ('e', 's')\n",
            "Tokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 8, 'r': 2, 'n': 6, 'es': 9, 't': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 11\n",
            "==========\n",
            "es\\ t\n",
            "re.compile('(?<!\\\\S)es\\\\ t(?!\\\\S)')\n",
            "l o w </w>\n",
            "l o w e r </w>\n",
            "n e w est </w>\n",
            "w i d est </w>\n",
            "Iter: 1\n",
            "Best pair: ('es', 't')\n",
            "Tokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 8, 'r': 2, 'n': 6, 'est': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 10\n",
            "==========\n",
            "est\\ \\<\\/w\\>\n",
            "re.compile('(?<!\\\\S)est\\\\ \\\\<\\\\/w\\\\>(?!\\\\S)')\n",
            "l o w </w>\n",
            "l o w e r </w>\n",
            "n e w est</w>\n",
            "w i d est</w>\n",
            "Iter: 2\n",
            "Best pair: ('est', '</w>')\n",
            "Tokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'est</w>': 9, 'i': 3, 'd': 3})\n",
            "Number of tokens: 10\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDNhMI3B7QE3"
      },
      "source": [
        "# vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
        "\n",
        "# Get free book from Gutenberg\n",
        "# wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
        "vocab = get_vocab('pg16457.txt')\n",
        "\n",
        "print('==========')\n",
        "print('Tokens Before BPE')\n",
        "tokens = get_tokens(vocab)\n",
        "print('Tokens: {}'.format(tokens))\n",
        "print('Number of tokens: {}'.format(len(tokens)))\n",
        "print('==========')\n",
        "\n",
        "num_merges = 1000\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print('Iter: {}'.format(i))\n",
        "    print('Best pair: {}'.format(best))\n",
        "    tokens = get_tokens(vocab)\n",
        "    print('Tokens: {}'.format(tokens))\n",
        "    print('Number of tokens: {}'.format(len(tokens)))\n",
        "    print('==========')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtSMYRPvE_29"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHoUedHgFAYj"
      },
      "source": [
        "### Encoding + Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOdV3g8yE_0V"
      },
      "source": [
        "import re, collections\n",
        "\n",
        "def get_vocab(filename):\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
        "        for line in fhand:\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def get_tokens_from_vocab(vocab):\n",
        "    tokens_frequencies = collections.defaultdict(int)\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "def measure_token_length(token):\n",
        "    if token[-4:] == '</w>':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)\n",
        "\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    print('TTT', sorted_tokens)\n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    string_tokens = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        token_reg = re.escape(token.replace('.', '[.]'))\n",
        "\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
        "        #print('matched_positions', matched_positions)\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
        "\n",
        "        substring_start_position = 0\n",
        "        for substring_end_position in substring_end_positions:\n",
        "            substring = string[substring_start_position:substring_end_position]\n",
        "            #print(substring)\n",
        "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "            string_tokens += [token]\n",
        "            #print(token)\n",
        "            substring_start_position = substring_end_position + len(token)\n",
        "        remaining_substring = string[substring_start_position:]\n",
        "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "        break\n",
        "    return string_tokens\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1nwoBF6FpD2",
        "outputId": "9bafb1a5-0832-4dd6-96e1-13ef8b97e8b4"
      },
      "source": [
        "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
        "print('==========')\n",
        "print('Tokens Before BPE')\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "print('==========')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========\n",
            "Tokens Before BPE\n",
            "All tokens: dict_keys(['l', 'o', 'w', '</w>', 'e', 'r', 'n', 's', 't', 'i', 'd'])\n",
            "Number of tokens: 11\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw2i_uCJFpBf",
        "outputId": "cf9b4496-ef28-483d-bb68-9ad60ca4c872"
      },
      "source": [
        "tokens_frequencies, vocab_tokenization"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(defaultdict(int,\n",
              "             {'</w>': 16,\n",
              "              'd': 3,\n",
              "              'e': 17,\n",
              "              'i': 3,\n",
              "              'l': 7,\n",
              "              'n': 6,\n",
              "              'o': 7,\n",
              "              'r': 2,\n",
              "              's': 9,\n",
              "              't': 9,\n",
              "              'w': 16}),\n",
              " {'low</w>': ['l', 'o', 'w', '</w>'],\n",
              "  'lower</w>': ['l', 'o', 'w', 'e', 'r', '</w>'],\n",
              "  'newest</w>': ['n', 'e', 'w', 'e', 's', 't', '</w>'],\n",
              "  'widest</w>': ['w', 'i', 'd', 'e', 's', 't', '</w>']})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K04jyWwaFEfB",
        "outputId": "4b8306df-c769-4e45-8030-9f7496b87697"
      },
      "source": [
        "print('==========')\n",
        "print('Tokens Before BPE')\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "print('==========')\n",
        "\n",
        "num_merges = 3 # 10000\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print('Iter: {}'.format(i))\n",
        "    print('Best pair: {}'.format(best))\n",
        "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
        "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "    print('==========')\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========\n",
            "Tokens Before BPE\n",
            "All tokens: dict_keys(['l', 'o', 'w', '</w>', 'e', 'r', 'n', 's', 't', 'i', 'd'])\n",
            "Number of tokens: 11\n",
            "==========\n",
            "Iter: 0\n",
            "Best pair: ('e', 's')\n",
            "All tokens: dict_keys(['l', 'o', 'w', '</w>', 'e', 'r', 'n', 'es', 't', 'i', 'd'])\n",
            "Number of tokens: 11\n",
            "==========\n",
            "Iter: 1\n",
            "Best pair: ('es', 't')\n",
            "All tokens: dict_keys(['l', 'o', 'w', '</w>', 'e', 'r', 'n', 'est', 'i', 'd'])\n",
            "Number of tokens: 10\n",
            "==========\n",
            "Iter: 2\n",
            "Best pair: ('est', '</w>')\n",
            "All tokens: dict_keys(['l', 'o', 'w', '</w>', 'e', 'r', 'n', 'est</w>', 'i', 'd'])\n",
            "Number of tokens: 10\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLPesyAxGrSL",
        "outputId": "7148751c-9c7a-4ab9-ea87-6171ad4b41c8"
      },
      "source": [
        "# Let's check how tokenization will be for a known word\n",
        "word_given_known = 'mountains</w>'\n",
        "#word_given_known = 'JJJ'\n",
        "word_given_unknown = 'Ilikeeatingapples!</w>'\n",
        "\n",
        "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
        "\n",
        "print(sorted_tokens)\n",
        "\n",
        "word_given = word_given_known "
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['est</w>', 'w', 'e', 'l', 'o', '</w>', 'n', 'i', 'd', 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSe7G0T2G6sb",
        "outputId": "b2a6d707-94da-4c06-d6eb-635c382ac1fa"
      },
      "source": [
        "for i in vocab_tokenization:\n",
        "    print(i)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "low</w>\n",
            "lower</w>\n",
            "newest</w>\n",
            "widest</w>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Og9lOZGHGOL",
        "outputId": "dc072353-f33a-49e9-aecc-e5f598066ee0"
      },
      "source": [
        "print('Tokenizing word: {}...'.format(word_given))\n",
        "if word_given in vocab_tokenization:\n",
        "    print('Tokenization of the known word:')\n",
        "    print(vocab_tokenization[word_given])\n",
        "    print('Tokenization treating the known word as unknown:')\n",
        "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
        "else:\n",
        "    print('Tokenizating of the unknown word:')\n",
        "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing word: mountains</w>...\n",
            "Tokenizating of the unknown word:\n",
            "TTT ['est</w>', 'w', 'e', 'l', 'o', '</w>', 'n', 'i', 'd', 'r']\n",
            "matched_positions []\n",
            "matched_positions [(11, 12)]\n",
            "TTT ['e', 'l', 'o', '</w>', 'n', 'i', 'd', 'r']\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions [(1, 2)]\n",
            "TTT ['</w>', 'n', 'i', 'd', 'r']\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "TTT ['</w>', 'n', 'i', 'd', 'r']\n",
            "matched_positions []\n",
            "matched_positions [(1, 2), (5, 6)]\n",
            "TTT ['i', 'd', 'r']\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "TTT ['i', 'd', 'r']\n",
            "matched_positions [(2, 3)]\n",
            "TTT ['d', 'r']\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "TTT ['d', 'r']\n",
            "TTT ['i', 'd', 'r']\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "TTT ['e', 'l', 'o', '</w>', 'n', 'i', 'd', 'r']\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "matched_positions []\n",
            "['o', 'n', 'i', 'n', 'w']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP4FslR9IoQ_",
        "outputId": "7d980ee2-a0e1-4937-f03f-2b3e3fc856be"
      },
      "source": [
        "tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['o', 'n', 'i', 'n', 'w']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhIobMXcGLlM",
        "outputId": "9247e256-aec8-4002-c785-5f2065848f4a"
      },
      "source": [
        "word_given = word_given_unknown \n",
        "\n",
        "print('Tokenizing word: {}...'.format(word_given))\n",
        "if word_given in vocab_tokenization:\n",
        "    print('Tokenization of the known word:')\n",
        "    print(vocab_tokenization[word_given])\n",
        "    print('Tokenization treating the known word as unknown:')\n",
        "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
        "else:\n",
        "    print('Tokenizating of the unknown word:')\n",
        "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing word: Ilikeeatingapples!</w>...\n",
            "Tokenizating of the unknown word:\n",
            "['l', 'i', 'e', 'e', 'i', 'n', 'l', 'e', 'w']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}